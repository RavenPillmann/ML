# Ensemble Methods
- Bagging: Get each model to predict a value, then combine them in some way (averaging values or choosing the value that comes up most).
- Boosting: Choosing model that can work best on each individual task.
- Weak Learners: Models that are joined into a stronger model.
- Strong Learner: The combination of the weak learner models.
- In general, we just need these models to do slightly better than random chance at their task.

## Bagging
- We can take subsets of data a train on model on each subset (don't have to be mutually separated, just randomly choose). Then we can take all models into consideration in some way.

## Adaboosting
- 
